{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_884317/2978474962.py:4: DeprecationWarning: Please use `binary_opening` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  from scipy.ndimage.morphology import binary_opening\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tifffile\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.morphology import binary_opening\n",
    "from skimage import measure\n",
    "from skimage.morphology import disk\n",
    "import warnings\n",
    "from config import *\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Gtgrid:\n",
    "    def __init__(self, img_gt, img_baseline, area=200):\n",
    "        self.img_gt = img_gt\n",
    "        self.img_baseline = img_baseline\n",
    "        self.img_gt = self.remove_nuclei_border(self.img_gt)\n",
    "        self.img_baseline = self.remove_nuclei_border(self.img_baseline)\n",
    "\n",
    "    def remove_nuclei_border(self, img, margin=3):\n",
    "        uniques, counts = np.unique(img, return_counts=True)\n",
    "        for rep in uniques:\n",
    "            tmp = np.where(img == rep)\n",
    "            x_min, x_max = np.min(tmp[0]), np.max(tmp[0])\n",
    "            y_min, y_max = np.min(tmp[1]), np.max(tmp[1])\n",
    "            bool_erase = (\n",
    "                (x_min <= margin)\n",
    "                or (x_max >= img.shape[0] - margin)\n",
    "                or (y_min <= margin)\n",
    "                or (y_max >= img.shape[0] - margin)\n",
    "            )\n",
    "            bool_erase = bool_erase\n",
    "            if bool_erase:\n",
    "                img[img == rep] = 0\n",
    "        return img\n",
    "\n",
    "    def create_dictionnary(self):\n",
    "        dic = {}\n",
    "        \"\"\"\n",
    "        trouvons les associations en premier\n",
    "        \"\"\"\n",
    "        uniques_gt = np.unique(self.img_gt)[1:]\n",
    "        uniques_baseline = np.unique(self.img_baseline)[1:]\n",
    "        for unique_gt in uniques_gt:\n",
    "            for unique_baseline in uniques_baseline:\n",
    "                nuclei_baseline = self.img_baseline == unique_baseline\n",
    "                nuclei_gt = self.img_gt == unique_gt\n",
    "                iou = np.sum(nuclei_baseline * nuclei_gt) / np.sum(\n",
    "                    np.maximum(nuclei_baseline, nuclei_gt)\n",
    "                )\n",
    "\n",
    "                if iou > 0.5:\n",
    "                    dic[unique_gt] = unique_baseline\n",
    "\n",
    "        used_gt = list(dic.keys())\n",
    "        used_baseline = list(dic.values())\n",
    "        all_gts = list(uniques_gt)\n",
    "        all_baselines = list(uniques_baseline)\n",
    "        not_used_gt = np.array(list(set(all_gts) - set(used_gt)))\n",
    "        not_used_baseline = np.array(list(set(all_baselines) - set(used_baseline)))\n",
    "\n",
    "        \"\"\"cherchons si stardist n'a pas mergé des noyaux\"\"\"\n",
    "        not_used_baseline1 = not_used_baseline.copy()\n",
    "        not_used_gt1 = not_used_gt.copy()\n",
    "\n",
    "        for nb_nuclei_baseline1 in not_used_baseline1:\n",
    "            nuclei_baseline = self.img_baseline == nb_nuclei_baseline1\n",
    "            i = 0\n",
    "            l = []\n",
    "            for nb_nuclei_gt1 in not_used_gt1:\n",
    "                nucleit_gt = self.img_gt == nb_nuclei_gt1\n",
    "                iou = np.sum(nuclei_baseline * nuclei_gt) / np.sum(nuclei_gt)\n",
    "\n",
    "                if iou > 0.5:\n",
    "                    l.append(nb_nuclei_gt1)\n",
    "\n",
    "            if len(l) >= 2:\n",
    "                for element in l:\n",
    "                    dic[element] = nb_nuclei_baseline1\n",
    "\n",
    "                not_used_baseline1 = np.delete(\n",
    "                    not_used_baseline1,\n",
    "                    np.where(not_used_baseline1 == nb_nuclei_baseline1),\n",
    "                )\n",
    "                not_used_gt1 = np.delete(\n",
    "                    not_used_gt1, np.where(np.isin(not_used_gt1, l))[0].flatten()\n",
    "                )\n",
    "\n",
    "        not_used_baseline2 = not_used_baseline1.copy()\n",
    "        not_used_gt2 = not_used_gt1.copy()\n",
    "\n",
    "        \"\"\"cherchons si stardist n'a pas splité des noyaux\"\"\"\n",
    "        # print(not_used_baseline2)\n",
    "        for nb_nuclei_gt2 in not_used_gt2:\n",
    "            nuclei_gt = self.img_gt == nb_nuclei_gt2\n",
    "            l = []\n",
    "            for nb_nuclei_baseline2 in not_used_baseline2:\n",
    "                nuclei_baseline = self.img_baseline == nb_nuclei_baseline2\n",
    "                iou = np.sum(nuclei_gt * nuclei_baseline) / np.sum(nuclei_baseline)\n",
    "                if iou >= 0.5:\n",
    "                    # print('hey')\n",
    "                    # plt.imshow(nuclei_baseline)\n",
    "                    # plt.show()\n",
    "                    l.append(nb_nuclei_baseline2)\n",
    "\n",
    "            if len(l) >= 2:\n",
    "                dic[nb_nuclei_gt2] = l\n",
    "                not_used_gt2 = np.delete(\n",
    "                    not_used_gt2, np.where(not_used_gt2 == nb_nuclei_gt2)\n",
    "                )\n",
    "                not_used_baseline2 = np.delete(\n",
    "                    not_used_baseline2,\n",
    "                    np.where(np.isin(not_used_baseline2, l))[0].flatten(),\n",
    "                )\n",
    "\n",
    "        \"\"\"\n",
    "        if a nuclei appears several times in the values of the dictionnary,\n",
    "        the baseline has merged some gt nucleis\n",
    "        \"\"\"\n",
    "        values_dic = list(dic.values())\n",
    "        unique, counts = np.unique(self.flatten(values_dic), return_counts=True)\n",
    "        baseline_merges = []\n",
    "        for merge in np.where(counts > 1)[0].flatten():\n",
    "            baseline_merges.append(unique[merge])\n",
    "\n",
    "        \"\"\"\n",
    "        if a list is in the values of the dictionnary, the baseline splitted a nuclei_gt\n",
    "        \"\"\"\n",
    "\n",
    "        gt_splits = np.array([u for u in list(dic.keys()) if type(dic[u]) == list])\n",
    "        return (\n",
    "            np.array(baseline_merges).astype(int),\n",
    "            gt_splits.astype(int),\n",
    "            not_used_gt2.astype(int),\n",
    "            not_used_baseline2.astype(int),\n",
    "        )\n",
    "\n",
    "    def get_size_of_error(self, binary, margin=15):\n",
    "        indexes = np.argwhere(binary)\n",
    "        bottom_left = np.min(indexes, 0)\n",
    "        upper_right = np.max(indexes, 0)\n",
    "        return np.max(np.abs(bottom_left - upper_right)) + margin\n",
    "\n",
    "    def flatten(self, list_):\n",
    "        new_list = []\n",
    "        for v in list_:\n",
    "            if type(v) != list:\n",
    "                new_list.append(v)\n",
    "            else:\n",
    "                new_list = new_list + v\n",
    "        return new_list\n",
    "\n",
    "    def create_dic_errors(self):\n",
    "        all_errors = self.create_dictionnary()\n",
    "        centers = []\n",
    "        sizes = []\n",
    "        receptive_field_sizes = np.array([5, 13, 29, 61, 125])\n",
    "\n",
    "        keys = [\"merge\", \"split\", \"fn\", \"fp\"]\n",
    "        dic = {}\n",
    "        for key, errors, img in zip(\n",
    "            keys,\n",
    "            all_errors,\n",
    "            (self.img_baseline, self.img_gt, self.img_gt, self.img_baseline),\n",
    "        ):\n",
    "            scale1 = np.zeros((256, 256))\n",
    "            scale2 = np.zeros((128, 128))\n",
    "            scale3 = np.zeros((64, 64))\n",
    "            scale4 = np.zeros((32, 32))\n",
    "            scale5 = np.zeros((16, 16))\n",
    "\n",
    "            scales = [scale1, scale2, scale3, scale4, scale5]\n",
    "\n",
    "            for nb in errors:\n",
    "                error = img == nb\n",
    "                rows, columns = np.where(error)[0:2]\n",
    "                min_rows, max_rows = np.min(rows), np.max(rows)\n",
    "                min_columns, max_columns = np.min(columns), np.max(columns)\n",
    "                mean_rows = (max_rows + min_rows) // 2\n",
    "                mean_columns = (max_columns + min_columns) // 2\n",
    "                size = self.get_size_of_error(error)\n",
    "                print(size)\n",
    "                scale = np.searchsorted(receptive_field_sizes, size).astype(int)\n",
    "                factor_resize = 1 / (2 ** (scale))\n",
    "                row, col = np.round(factor_resize * mean_rows).astype(int), np.round(\n",
    "                    factor_resize * mean_columns\n",
    "                ).astype(int)\n",
    "                row, col = np.clip(row,0,(256//(2**scale))-1).astype(int), np.clip(col,0,(256/(2**scale))-1).astype(int)\n",
    "                print(scale,row,col)\n",
    "                scales[scale][row, col] = 1\n",
    "            scales = [u for u in scales if np.sum(u) > 0]\n",
    "            dic[key] = scales\n",
    "\n",
    "        all_scales_used = []\n",
    "        for key, value in dic.items():\n",
    "            for l in value:\n",
    "                all_scales_used.append(l.shape[0])\n",
    "        return dic, np.unique(all_scales_used)\n",
    "\n",
    "    def create_grid(self):\n",
    "        dic = self.create_dic_errors()[0]\n",
    "        u = np.zeros((256, 256, 4))\n",
    "        for i, (error, array) in enumerate(dic.items()):\n",
    "            u_error = np.zeros((256, 256))\n",
    "            if len(array) != 0:\n",
    "                for arr in array:\n",
    "                    factor_resize = 256 / arr.shape[0]\n",
    "                    coord_image = (\n",
    "                        (np.stack(np.where(arr)).T + 1 / 2) * factor_resize\n",
    "                    ).astype(int)\n",
    "                    for coord in coord_image:\n",
    "                        u_error[coord[0], coord[1]] = 1\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "3 14 25\n",
      "28\n",
      "2 26 59\n",
      "43\n",
      "3 12 27\n",
      "44\n",
      "3 18 20\n",
      "31\n",
      "3 27 28\n",
      "43\n",
      "3 21 23\n",
      "45\n",
      "3 7 10\n",
      "43\n",
      "3 4 8\n",
      "25\n",
      "2 60 52\n",
      "39\n",
      "3 19 26\n",
      "43\n",
      "3 8 26\n",
      "37\n",
      "3 28 29\n",
      "55\n",
      "3 16 4\n",
      "33\n",
      "3 30 20\n",
      "36\n",
      "3 3 4\n",
      "33\n",
      "3 17 15\n",
      "179\n",
      "5 2 4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ahabis/1-Click_project/test.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m img_baseline \u001b[39m=\u001b[39m tifffile\u001b[39m.\u001b[39mimread(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path_stardist_modified, filename))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m grid_creator \u001b[39m=\u001b[39m Gtgrid(img_gt, img_baseline, area\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m grid \u001b[39m=\u001b[39m grid_creator\u001b[39m.\u001b[39;49mcreate_grid()\n",
      "\u001b[1;32m/home/ahabis/1-Click_project/test.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_grid\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=200'>201</a>\u001b[0m     dic \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_dic_errors()[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=201'>202</a>\u001b[0m     u \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=202'>203</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (error, array) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dic\u001b[39m.\u001b[39mitems()):\n",
      "\u001b[1;32m/home/ahabis/1-Click_project/test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m     row, col \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(row,\u001b[39m0\u001b[39m,(\u001b[39m256\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscale))\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m), np\u001b[39m.\u001b[39mclip(col,\u001b[39m0\u001b[39m,(\u001b[39m256\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscale))\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m     \u001b[39mprint\u001b[39m(scale,row,col)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m     scales[scale][row, col] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m scales \u001b[39m=\u001b[39m [u \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m scales \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39msum(u) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdell1/home/ahabis/1-Click_project/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m dic[key] \u001b[39m=\u001b[39m scales\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "# from create_gt_grids import Gtgrid\n",
    "\n",
    "filename = 'nuclei_3_1351.tif'\n",
    "img_gt = tifffile.imread(os.path.join(path_gt, filename))\n",
    "img_baseline = tifffile.imread(os.path.join(path_stardist_modified, filename))\n",
    "grid_creator = Gtgrid(img_gt, img_baseline, area=0)\n",
    "grid = grid_creator.create_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
